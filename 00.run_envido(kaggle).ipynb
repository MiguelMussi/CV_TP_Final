{"metadata":{"colab":{"provenance":[],"collapsed_sections":["fHADhl4_hkYp","FglvzTrXhkYt","kTG_5YDThkYu","vAkTjCfUhkYv"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9076399,"sourceType":"datasetVersion","datasetId":5475460}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **TUIA - Procesamiento de Imágenes y Visión por Computadora (IA52)**\n# **Trabajo Práctico Final**\n### **Ejercicio 4 - Evaluación**\n<br>\n\n### *Alumno: Miguel Mussi*\n### *Año: 2024*\n\n---------------------\n## **Tabla de contenidos**\n1.   [**Librerías**](#)\n2.   [**Ajustes Iniciales**](#)\n3.   [**Optimización de inferencia**](#)\n4.   [**Inferencia sobre las imágenes y cálculo del envido**](#)\n        1.   [*Inferencia con CPU*](#)\n        2.   [*Inferencia con GPU - Sin TensorRT*](#)\n        3.   [*Inferencia con GPU - Con TensorRT*](#)\n        4.   [*Comparativa de Inferencias*](#)\n5.   [**Guardamos las imagenes con las predicciones**](#)\n6.   [**Escritura del archivo envido.json**](#)\n7.   [**Exportar directorios**](#)","metadata":{"id":"SUKX__a1UgGt"}},{"cell_type":"markdown","source":"## 1. **Librerías**","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"I9JftkdJhkYQ"}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"id":"OeICo45_hkYS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dcbaaca9-f59c-4dfa-ba17-566d80c8518f","execution":{"iopub.status.busy":"2024-07-31T17:17:50.246330Z","iopub.execute_input":"2024-07-31T17:17:50.246613Z","iopub.status.idle":"2024-07-31T17:18:05.896721Z","shell.execute_reply.started":"2024-07-31T17:17:50.246587Z","shell.execute_reply":"2024-07-31T17:18:05.895491Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.2.70-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m552.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.2.2)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.5.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nDownloading ultralytics-8.2.70-py3-none-any.whl (862 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.6/862.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\nInstalling collected packages: ultralytics-thop, ultralytics\nSuccessfully installed ultralytics-8.2.70 ultralytics-thop-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install onnx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjbG0RCXkgCx","outputId":"35ffba4f-d5f1-43c7-92fc-0949c834e401","execution":{"iopub.status.busy":"2024-07-31T17:18:05.906598Z","iopub.execute_input":"2024-07-31T17:18:05.906863Z","iopub.status.idle":"2024-07-31T17:18:18.101190Z","shell.execute_reply.started":"2024-07-31T17:18:05.906834Z","shell.execute_reply":"2024-07-31T17:18:18.100135Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (1.16.1)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from onnx) (1.26.4)\nRequirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from onnx) (3.20.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport json\nimport torch\nimport shutil\nfrom ultralytics import YOLO\nimport matplotlib.pyplot as plt","metadata":{"id":"ZxlBx8OXUgGu","execution":{"iopub.status.busy":"2024-07-31T17:18:21.698952Z","iopub.execute_input":"2024-07-31T17:18:21.699633Z","iopub.status.idle":"2024-07-31T17:18:26.252693Z","shell.execute_reply.started":"2024-07-31T17:18:21.699599Z","shell.execute_reply":"2024-07-31T17:18:26.251721Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"Z5wXHxYThkYY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9a7aaec-445b-4934-d60e-7416e7230954"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/drive\n"}]},{"cell_type":"markdown","source":"## 2. **Ajustes Iniciales**","metadata":{"id":"Kzf_37QPUgGu","jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# Nombre del alumno\nstudent_name = 'miguel_mussi'\n\n# Ruta al directorio principal\nmain_path = '/kaggle/input/cv-tp-final/CV_TP_Final'\n\n# Ruta al archivo de pesos\nmodel_path = os.path.join(main_path, 'model/weights/best.pt')\n# model_path_tensor = os.path.join(main_path, '/kaggle/working/model/weights/best.onnx')\nmodel_path_tensor = os.path.join('/kaggle/working', 'best.onnx')\n\n# Ruta al directorio que contiene las imagenes\nimgs_dir = os.path.join(main_path, 'data/eval/images/val')\n\n# Ruta al directorio de destino de las detecciones\nbase_dir = os.path.join('/kaggle/working/data/out/')\ndets_dir = os.path.join(base_dir, student_name)\n\n# Reestablecimiento del directorio de destino (eliminacion)\nif os.path.exists(dets_dir):\n    shutil.rmtree(dets_dir)\nos.makedirs(dets_dir)","metadata":{"id":"LrsYEZXHUgGv","execution":{"iopub.status.busy":"2024-07-31T17:41:40.862781Z","iopub.execute_input":"2024-07-31T17:41:40.863411Z","iopub.status.idle":"2024-07-31T17:41:40.870219Z","shell.execute_reply.started":"2024-07-31T17:41:40.863376Z","shell.execute_reply":"2024-07-31T17:41:40.869255Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Nombre del alumno\nstudent_name = 'miguel_mussi'\n\n# Ruta al directorio principal\nmain_path = '/kaggle/input/cv-tp-final/CV_TP_Final'\n\n# Ruta del directorio de trabajo\nworking_dir = '/kaggle/working/'\n\n# Ruta al archivo de pesos copiado en el directorio de trabajo\nmodel_path_working = os.path.join(working_dir, 'model/weights/best.pt')\n\n# Copiar la carpeta 'model' al directorio de trabajo\nif not os.path.exists(os.path.join(working_dir, 'model')):\n    shutil.copytree(os.path.join(main_path, 'model'), os.path.join(working_dir, 'model'))\n\n# Ruta para guardar el modelo exportado\nmodel_path_tensor = os.path.join(working_dir, 'model/weights/best.onnx')\n\n# Ruta al directorio que contiene las imágenes\nimgs_dir = os.path.join(main_path, 'data/eval/images/val')\n\n# Ruta al directorio de destino de las detecciones\nbase_dir = os.path.join(working_dir, 'data/out/')\ndets_dir = os.path.join(base_dir, student_name)\n\n# Reestablecimiento del directorio de destino (eliminación)\nif os.path.exists(dets_dir):\n    shutil.rmtree(dets_dir)\nos.makedirs(dets_dir)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T18:00:18.734082Z","iopub.execute_input":"2024-07-31T18:00:18.734480Z","iopub.status.idle":"2024-07-31T18:00:18.743151Z","shell.execute_reply.started":"2024-07-31T18:00:18.734449Z","shell.execute_reply":"2024-07-31T18:00:18.742254Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"if os.path.exists('/kaggle/working/model'):\n    shutil.rmtree('/kaggle/working/model')\n# os.makedirs('/kaggle/working/model')","metadata":{"execution":{"iopub.status.busy":"2024-07-31T17:58:01.225653Z","iopub.execute_input":"2024-07-31T17:58:01.226350Z","iopub.status.idle":"2024-07-31T17:58:01.246593Z","shell.execute_reply.started":"2024-07-31T17:58:01.226315Z","shell.execute_reply":"2024-07-31T17:58:01.245863Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Seteo de tiempos de ejecución\nexecution_time = {'cpu': 0, 'gpu': 0, 'rt': 0}","metadata":{"id":"yu8PD-WRsn2h","execution":{"iopub.status.busy":"2024-07-31T17:21:53.141126Z","iopub.execute_input":"2024-07-31T17:21:53.141814Z","iopub.status.idle":"2024-07-31T17:21:53.146082Z","shell.execute_reply.started":"2024-07-31T17:21:53.141785Z","shell.execute_reply":"2024-07-31T17:21:53.145060Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## 3. **Optimización de inferencia**","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"0pBqm9G3hkYd"}},{"cell_type":"code","source":"# Cargamos el modelo\nmodel = YOLO(model_path_working)","metadata":{"id":"m0ucZWDi4uAt","execution":{"iopub.status.busy":"2024-07-31T17:58:11.789911Z","iopub.execute_input":"2024-07-31T17:58:11.790282Z","iopub.status.idle":"2024-07-31T17:58:11.947046Z","shell.execute_reply.started":"2024-07-31T17:58:11.790247Z","shell.execute_reply":"2024-07-31T17:58:11.946056Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Lo exportamos como un tensor para utilizar tensorRT con la GPU\nmodel.export(format='onnx', imgsz=640, dynamic=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"VNAwXVLhDeT7","outputId":"4783bb65-6482-4e2e-c023-93e108231b84","execution":{"iopub.status.busy":"2024-07-31T18:00:27.263083Z","iopub.execute_input":"2024-07-31T18:00:27.263873Z","iopub.status.idle":"2024-07-31T18:00:35.313440Z","shell.execute_reply.started":"2024-07-31T18:00:27.263842Z","shell.execute_reply":"2024-07-31T18:00:35.312306Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.70 🚀 Python-3.10.13 torch-2.1.2 CPU (Intel Xeon 2.00GHz)\nModel summary (fused): 268 layers, 43,644,387 parameters, 0 gradients, 165.0 GFLOPs\n\n\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/kaggle/working/model/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 53, 8400) (83.6 MB)\n\n\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.1 opset 17...\n\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 3.8s, saved as '/kaggle/working/model/weights/best.onnx' (166.6 MB)\n\nExport complete (8.0s)\nResults saved to \u001b[1m/kaggle/working/model/weights\u001b[0m\nPredict:         yolo predict task=detect model=/kaggle/working/model/weights/best.onnx imgsz=640  \nValidate:        yolo val task=detect model=/kaggle/working/model/weights/best.onnx imgsz=640 data=/kaggle/working/dataset.yaml  \nVisualize:       https://netron.app\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/model/weights/best.onnx'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Comprobación de entorno gráfico","metadata":{"id":"YhBmvKuS3e8I"}},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"id":"MfZDDGtnp7zA","outputId":"c74c43fe-59ba-4187-aa40-c57824f929d9","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-31T18:02:30.856771Z","iopub.execute_input":"2024-07-31T18:02:30.857243Z","iopub.status.idle":"2024-07-31T18:02:30.862476Z","shell.execute_reply.started":"2024-07-31T18:02:30.857210Z","shell.execute_reply":"2024-07-31T18:02:30.861469Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(f'Usando el dispositivo: {device}')","metadata":{"id":"SI-9xjbZ4NWE","execution":{"iopub.status.busy":"2024-07-31T17:25:33.490125Z","iopub.execute_input":"2024-07-31T17:25:33.490476Z","iopub.status.idle":"2024-07-31T17:25:33.494757Z","shell.execute_reply.started":"2024-07-31T17:25:33.490449Z","shell.execute_reply":"2024-07-31T17:25:33.493694Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(f'Dispositivo del modelo: {next(model.parameters()).device}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7CgC6KSX4Qki","outputId":"6fe1689f-fe39-4df6-c851-87948e073f1f","execution":{"iopub.status.busy":"2024-07-31T17:25:23.562126Z","iopub.execute_input":"2024-07-31T17:25:23.562599Z","iopub.status.idle":"2024-07-31T17:25:23.567938Z","shell.execute_reply.started":"2024-07-31T17:25:23.562567Z","shell.execute_reply":"2024-07-31T17:25:23.566782Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Dispositivo del modelo: cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4wLIyEkD5-k6","outputId":"ee1717ea-59cc-42c4-f04d-1d4c6ff9113e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\n\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n\nRequirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"}]},{"cell_type":"code","source":"# device_id = 0  # El ID de dispositivo que deseas usar\n# if device_id < torch.cuda.device_count():\n#     device = torch.device(f'cuda:{device_id}')\n# else:\n#     raise AssertionError(\"Invalid device id\")","metadata":{"id":"inEsYn2a5n7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\nprint(f'Número de GPUs disponibles: {num_gpus}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sLIDDDuT5ZZr","outputId":"b2ad0851-e798-4a95-f8df-5ffa6b7923ce","execution":{"iopub.status.busy":"2024-07-31T18:02:37.210573Z","iopub.execute_input":"2024-07-31T18:02:37.211170Z","iopub.status.idle":"2024-07-31T18:02:37.216059Z","shell.execute_reply.started":"2024-07-31T18:02:37.211135Z","shell.execute_reply":"2024-07-31T18:02:37.214991Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Número de GPUs disponibles: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 4. **Inferencia sobre las imágenes y cálculo del envido**","metadata":{"id":"MEvz0ZGdUgGw","jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"def calcular_envido(num1,num2):\n    n1 = int(num1)\n    n2 = int(num2)\n    ret = n1 if n1<10 else 0\n    ret += n2 if n2<10 else 0\n    return ret + 20","metadata":{"id":"lE7E_TxmpTGn","execution":{"iopub.status.busy":"2024-07-31T17:25:56.593100Z","iopub.execute_input":"2024-07-31T17:25:56.593465Z","iopub.status.idle":"2024-07-31T17:25:56.598722Z","shell.execute_reply.started":"2024-07-31T17:25:56.593437Z","shell.execute_reply":"2024-07-31T17:25:56.597530Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### 4.1. *Inferencia con CPU*","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"wY0if1KUhkYn"}},{"cell_type":"code","source":"# Marca de tiempo antes de la inferencia\nstart_time = time.time()\n\n# Cargamos el modelo\ndevice = torch.device('cpu')\nmodel = YOLO(model_path).to(device)\n\n# Ejecutamos la inferencia\nresults = model(imgs_dir, stream=True)\n\n# Diccionario que guardará la información para el JSON\ncard_js_file = {}\n\n# Umbral de confianza para las detecciones\ndetection_threshold = 0.5\n\n# Iteramos sobre los resultados (archivos)\nfor result in results:\n    # Obtenemos el nombre del archivo de imagen\n    img_filename = os.path.basename(result.path)\n\n    # Contador de cartas inválidas y envido\n    invalid_cards = 0\n\n    # Diccionario para almacenar los tipos de cartas\n    card_types = {'O': [], 'C': [], 'E': [], 'B': []}\n\n    # Obtenemos las cartas detectadas\n    cards = result.cpu()\n\n    # Guardamos las detecciones en un archivo de texto en formato YOLOv5\n    cards.save_txt(os.path.join(dets_dir, img_filename.replace('.jpg', '.txt')), save_conf=True)\n\n    # Filtrar detecciones con umbral de confianza inferior a 0.45\n    filtered_cards = [box for box in result.boxes if box.conf.item() >= detection_threshold]\n\n    # Procesamos cada carta detectada\n    for card in filtered_cards:\n        # Obtenemos el nombre de la carta\n        # name = filtered_cards.names[int(card.boxes.cls)]\n        name = cards.names[int(card.cls)]\n\n        # Separamos el número y el palo de la carta\n        if len(name) == 3:      # (Carta negra. Ejemplo 10E - 11E - 12B )\n            num = name[0] + name[1]  # número de la carta negra\n            palo = name[2]           # palo de la carta\n        elif len(name) == 2:                   # (Carta blanca. Ejemplo xO - xC - xE - xB )\n            num = name[0]            # número de la carta\n            palo = name[1]           # palo de la carta\n        else:\n            num = name[0]            # número del comodín\n            palo = 'N/A'             # palo del comodín\n\n        # Comprobamos si la carta es inválida\n        if num in ['8', '9', 'J']:\n            invalid_cards += 1\n\n        # Agregamos la carta al diccionario correspondiente\n        if palo in card_types:\n            if num not in card_types[palo]:\n                card_types[palo].append(num)\n            else:\n                print(\"Se encontraron cartas repetidas\")\n                invalid_cards += 1\n\n    envido = {}\n\n    # Comprobar si la cantidad de cartas es 3\n    if len(filtered_cards) != 3 or invalid_cards:\n        print('La cantidad de cartas no permite calcular el envido.\\n')\n\n    # 3 cartas en la detección\n    else:\n        # Inicializamos la variable palo\n\n        # calculamos el envido\n        # Iteramos en el diccionario - Key = palos / Value: números\n        for key, values in card_types.items():\n            envido[key]=0\n            # Dos cartas del mismo palo\n            if len(values) == 1:\n                envido[key] = int(num) if len(num)==1 else 0\n\n            elif len(values) == 2:\n                envido[key] = calcular_envido(values[0],values[1])\n\n            # Tres cartas del mismo palo\n            elif len(values) == 3:\n                print('Mano con Flor.\\n')\n                envido[key] = calcular_envido(values[0],values[1])\n                envido[key] = max(envido[key],calcular_envido(values[0],values[2]))\n                envido[key] = max(envido[key],calcular_envido(values[1],values[2]))\n\n\n    mejor_palo = 'N/A'\n    mejor_punto = 0\n\n    for palo, punto in envido.items():\n        if punto > mejor_punto:\n            mejor_punto=punto\n            mejor_palo = palo\n\n    # Almacenamos los resultados del envido en el diccionario para el JSON\n\n\n    card_js_file[img_filename] = {\n            'total_cards': len(filtered_cards),\n            'cards': card_types,\n            'points': mejor_punto,       # Puntos de envido\n            'figure': mejor_palo          # Palo\n        }\n\n\n# Marca de tiempo después de la inferencia\nend_time = time.time()\n\n# Calculamos el tiempo de ejecución\nexecution_time['cpu'] = end_time - start_time\n\n# Mostramos que dispositivo se esta utilizando y el tiempo de ejecución\nif device.type == 'cuda': print('\\nEl modelo está utilizando la GPU.')\nelse: print('\\nEl modelo está utilizando la CPU.')\n# print(f'Tiempo de ejecución: {execution_time['cpu']:.2f} segundos')\nprint(f\"Tiempo de ejecución: {execution_time['cpu']:.2f} segundos\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ED5w3buhGJO_","outputId":"0c279c6a-7402-4dcc-932c-d0e800deb3a6","execution":{"iopub.status.busy":"2024-07-31T17:26:51.489640Z","iopub.execute_input":"2024-07-31T17:26:51.490312Z","iopub.status.idle":"2024-07-31T17:27:26.146556Z","shell.execute_reply.started":"2024-07-31T17:26:51.490281Z","shell.execute_reply":"2024-07-31T17:27:26.145864Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\nimage 1/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173711753_HDR.jpg: 640x512 1 4O, 1 11C, 960.6ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 2/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173737533.jpg: 640x512 1 1C, 1 2B, 1 5C, 936.6ms\nimage 3/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173758691.jpg: 640x512 1 1C, 1 2B, 947.4ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 4/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173822184_HDR.jpg: 640x512 1 3B, 1 6B, 1 7E, 947.2ms\nimage 5/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173828513_HDR.jpg: 640x512 1 3B, 1 6B, 1 7E, 1 J, 924.7ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 6/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173836085.jpg: 640x512 1 6B, 1 7E, 919.5ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 7/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173859456.jpg: 640x512 1 3B, 916.5ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 8/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173918579.jpg: 640x512 1 8O, 1 9O, 898.6ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 9/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173929084.jpg: 640x512 1 3O, 1 8O, 1 9O, 906.1ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 10/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173954348.jpg: 640x512 1 1O, 1 10C, 920.8ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 11/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174005265.jpg: 640x512 1 1O, 885.4ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 12/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174017809.jpg: 640x512 1 6E, 1 8B, 1 9B, 926.0ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 13/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174105714.jpg: 640x512 1 8B, 903.9ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 14/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174115784.jpg: 640x512 1 8C, 1 9C, 1 9E, 917.7ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 15/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174133804_HDR.jpg: 640x512 1 3E, 1 4E, 914.0ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 16/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174142929_HDR.jpg: 640x512 1 3E, 887.5ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 17/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174155383_HDR.jpg: 640x512 1 11E, 1 12E, 913.9ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 18/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174208092.jpg: 640x512 1 5E, 1 11O, 921.1ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 19/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174223161_HDR.jpg: 640x512 1 4O, 1 12B, 902.7ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 20/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174235296.jpg: 640x512 1 4O, 1 10E, 896.4ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 21/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174246178_HDR.jpg: 640x512 1 2C, 898.7ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 22/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174302382_HDR.jpg: 640x512 1 1E, 1 6C, 900.7ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 23/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174316833.jpg: 640x512 1 2O, 1 7C, 911.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 24/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174333447.jpg: 640x512 1 1B, 1 4B, 1 7B, 918.7ms\nMano con Flor.\n\nimage 25/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174349395.jpg: 640x512 1 7O, 1 10B, 913.1ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 26/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174406256.jpg: 640x512 2 5Os, 1 10B, 933.6ms\nSe encontraron cartas repetidas\nLa cantidad de cartas no permite calcular el envido.\n\nimage 27/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174418639.jpg: 640x512 2 5Os, 1 6O, 1 10B, 909.4ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 28/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174434126.jpg: 640x512 1 7C, 1 12O, 968.4ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 29/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174445046_HDR.jpg: 640x512 1 6O, 950.9ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 30/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174453153.jpg: 640x512 1 1B, 1 3C, 942.2ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 31/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174459285.jpg: 512x640 1 5B, 947.0ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 32/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174514649_HDR.jpg: 640x512 1 10O, 1 11B, 1 12C, 927.3ms\nSpeed: 3.5ms preprocess, 920.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n\nEl modelo está utilizando la CPU.\nTiempo de ejecución: 34.64 segundos\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4.2. *Inferencia con GPU - Sin TensorRT*","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"fHADhl4_hkYp"}},{"cell_type":"code","source":"# Marca de tiempo antes de la inferencia\nstart_time = time.time()\n\n# Cargamos el modelo\n# device = torch.device('cuda:0')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = YOLO(model_path).to(device)\n\n# Ejecutamos la inferencia\nresults = model(imgs_dir, stream=True)\n\n# Diccionario que guardará la información para el JSON\ncard_js_file = {}\n\n# Umbral de confianza para las detecciones\ndetection_threshold = 0.5\n\n# Iteramos sobre los resultados (archivos)\nfor result in results:\n    # Obtenemos el nombre del archivo de imagen\n    img_filename = os.path.basename(result.path)\n\n    # Contador de cartas inválidas y envido\n    invalid_cards = 0\n\n    # Diccionario para almacenar los tipos de cartas\n    card_types = {'O': [], 'C': [], 'E': [], 'B': []}\n\n    # Obtenemos las cartas detectadas\n    cards = result.cpu()\n\n    # Guardamos las detecciones en un archivo de texto en formato YOLOv5\n    cards.save_txt(os.path.join(dets_dir, img_filename.replace('.jpg', '.txt')), save_conf=True)\n\n    # Filtrar detecciones con umbral de confianza inferior a 0.45\n    filtered_cards = [box for box in result.boxes if box.conf.item() >= detection_threshold]\n\n    # Procesamos cada carta detectada\n    for card in filtered_cards:\n        # Obtenemos el nombre de la carta\n        # name = filtered_cards.names[int(card.boxes.cls)]\n        name = cards.names[int(card.cls)]\n\n        # Separamos el número y el palo de la carta\n        if len(name) == 3:      # (Carta negra. Ejemplo 10E - 11E - 12B )\n            num = name[0] + name[1]  # número de la carta negra\n            palo = name[2]           # palo de la carta\n        elif len(name) == 2:                   # (Carta blanca. Ejemplo xO - xC - xE - xB )\n            num = name[0]            # número de la carta\n            palo = name[1]           # palo de la carta\n        else:\n            num = name[0]            # número del comodín\n            palo = 'N/A'             # palo del comodín\n\n        # Comprobamos si la carta es inválida\n        if num in ['8', '9', 'J']:\n            invalid_cards += 1\n\n        # Agregamos la carta al diccionario correspondiente\n        if palo in card_types:\n            if num not in card_types[palo]:\n                card_types[palo].append(num)\n            else:\n                print(\"Se encontraron cartas repetidas\")\n                invalid_cards += 1\n\n    envido = {}\n\n    # Comprobar si la cantidad de cartas es 3\n    if len(filtered_cards) != 3 or invalid_cards:\n        print('La cantidad de cartas no permite calcular el envido.\\n')\n\n    # 3 cartas en la detección\n    else:\n        # Inicializamos la variable palo\n\n        # calculamos el envido\n        # Iteramos en el diccionario - Key = palos / Value: números\n        for key, values in card_types.items():\n            envido[key]=0\n            # Dos cartas del mismo palo\n            if len(values) == 1:\n                envido[key] = int(num) if len(num)==1 else 0\n\n            elif len(values) == 2:\n                envido[key] = calcular_envido(values[0],values[1])\n\n            # Tres cartas del mismo palo\n            elif len(values) == 3:\n                print('Mano con Flor.\\n')\n                envido[key] = calcular_envido(values[0],values[1])\n                envido[key] = max(envido[key],calcular_envido(values[0],values[2]))\n                envido[key] = max(envido[key],calcular_envido(values[1],values[2]))\n\n\n    mejor_palo = 'N/A'\n    mejor_punto = 0\n\n    for palo, punto in envido.items():\n        if punto > mejor_punto:\n            mejor_punto=punto\n            mejor_palo = palo\n\n    # Almacenamos los resultados del envido en el diccionario para el JSON\n\n\n    card_js_file[img_filename] = {\n            'total_cards': len(filtered_cards),\n            'cards': card_types,\n            'points': mejor_punto,       # Puntos de envido\n            'figure': mejor_palo          # Palo\n        }\n\n\n# Marca de tiempo después de la inferencia\nend_time = time.time()\n\n# Calculamos el tiempo de ejecución\nexecution_time['gpu'] = end_time - start_time\n\n# Mostramos que dispositivo se esta utilizando y el tiempo de ejecución\nif device.type == 'cuda': print('\\nEl modelo está utilizando la GPU.')\nelse: print('\\nEl modelo está utilizando la CPU.')\n# print(f'Tiempo de ejecución: {execution_time['cpu']:.2f} segundos')\nprint(f\"Tiempo de ejecución: {execution_time['gpu']:.2f} segundos\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pwH2UlYOu4uZ","outputId":"5d083849-eee8-44fe-c9ba-d94bb9837132","execution":{"iopub.status.busy":"2024-07-31T17:30:25.840084Z","iopub.execute_input":"2024-07-31T17:30:25.840510Z","iopub.status.idle":"2024-07-31T17:30:32.132729Z","shell.execute_reply.started":"2024-07-31T17:30:25.840478Z","shell.execute_reply":"2024-07-31T17:30:32.131858Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"\nimage 1/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173711753_HDR.jpg: 640x512 1 4O, 1 11C, 48.1ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 2/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173737533.jpg: 640x512 1 1C, 1 2B, 1 5C, 47.3ms\nimage 3/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173758691.jpg: 640x512 1 1C, 1 2B, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 4/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173822184_HDR.jpg: 640x512 1 3B, 1 6B, 1 7E, 47.2ms\nimage 5/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173828513_HDR.jpg: 640x512 1 3B, 1 6B, 1 7E, 1 J, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 6/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173836085.jpg: 640x512 1 6B, 1 7E, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 7/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173859456.jpg: 640x512 1 3B, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 8/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173918579.jpg: 640x512 1 8O, 1 9O, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 9/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173929084.jpg: 640x512 1 3O, 1 8O, 1 9O, 47.4ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 10/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173954348.jpg: 640x512 1 1O, 1 10C, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 11/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174005265.jpg: 640x512 1 1O, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 12/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174017809.jpg: 640x512 1 6E, 1 8B, 1 9B, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 13/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174105714.jpg: 640x512 1 8B, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 14/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174115784.jpg: 640x512 1 8C, 1 9C, 1 9E, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 15/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174133804_HDR.jpg: 640x512 1 3E, 1 4E, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 16/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174142929_HDR.jpg: 640x512 1 3E, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 17/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174155383_HDR.jpg: 640x512 1 11E, 1 12E, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 18/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174208092.jpg: 640x512 1 5E, 1 11O, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 19/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174223161_HDR.jpg: 640x512 1 4O, 1 12B, 47.4ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 20/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174235296.jpg: 640x512 1 4O, 1 10E, 47.4ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 21/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174246178_HDR.jpg: 640x512 1 2C, 47.4ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 22/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174302382_HDR.jpg: 640x512 1 1E, 1 6C, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 23/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174316833.jpg: 640x512 1 2O, 1 7C, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 24/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174333447.jpg: 640x512 1 1B, 1 4B, 1 7B, 47.3ms\nMano con Flor.\n\nimage 25/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174349395.jpg: 640x512 1 7O, 1 10B, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 26/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174406256.jpg: 640x512 2 5Os, 1 10B, 47.3ms\nSe encontraron cartas repetidas\nLa cantidad de cartas no permite calcular el envido.\n\nimage 27/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174418639.jpg: 640x512 2 5Os, 1 6O, 1 10B, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 28/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174434126.jpg: 640x512 1 7C, 1 12O, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 29/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174445046_HDR.jpg: 640x512 1 6O, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 30/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174453153.jpg: 640x512 1 1B, 1 3C, 47.3ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 31/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174459285.jpg: 512x640 1 5B, 48.6ms\nLa cantidad de cartas no permite calcular el envido.\n\nimage 32/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174514649_HDR.jpg: 640x512 1 10O, 1 11B, 1 12C, 44.4ms\nSpeed: 3.2ms preprocess, 47.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n\nEl modelo está utilizando la GPU.\nTiempo de ejecución: 6.27 segundos\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4.3. *Inferencia con GPU - Con TensorRT*","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"FglvzTrXhkYt"}},{"cell_type":"code","source":"# Cargamos el modelo\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Marca de tiempo antes de la inferencia\nstart_time = time.time()\n\n# Ejecutamos la inferencia\nresults = YOLO(model_path_tensor).predict(source=imgs_dir, device=device)\n\n# Diccionario que guardará la información para el JSON\ncard_js_file = {}\n\n# Umbral de confianza para las detecciones\ndetection_threshold = 0.5\n\n# Iteramos sobre los resultados (archivos)\nfor result in results:\n    # Obtenemos el nombre del archivo de imagen\n    img_filename = os.path.basename(result.path)\n\n    # Contador de cartas inválidas y envido\n    invalid_cards = 0\n\n    # Diccionario para almacenar los tipos de cartas\n    card_types = {'O': [], 'C': [], 'E': [], 'B': []}\n\n    # Obtenemos las cartas detectadas\n    cards = result.cpu()\n\n    # Guardamos las detecciones en un archivo de texto en formato YOLOv5\n    cards.save_txt(os.path.join(dets_dir, img_filename.replace('.jpg', '.txt')), save_conf=True)\n\n    # Filtrar detecciones con umbral de confianza inferior a 0.45\n    filtered_cards = [box for box in result.boxes if box.conf.item() >= detection_threshold]\n\n    # Procesamos cada carta detectada\n    for card in filtered_cards:\n        # Obtenemos el nombre de la carta\n        # name = filtered_cards.names[int(card.boxes.cls)]\n        name = cards.names[int(card.cls)]\n\n        # Separamos el número y el palo de la carta\n        if len(name) == 3:      # (Carta negra. Ejemplo 10E - 11E - 12B )\n            num = name[0] + name[1]  # número de la carta negra\n            palo = name[2]           # palo de la carta\n        elif len(name) == 2:                   # (Carta blanca. Ejemplo xO - xC - xE - xB )\n            num = name[0]            # número de la carta\n            palo = name[1]           # palo de la carta\n        else:\n            num = name[0]            # número del comodín\n            palo = 'N/A'             # palo del comodín\n\n        # Comprobamos si la carta es inválida\n        if num in ['8', '9', 'J']:\n            invalid_cards += 1\n\n        # Agregamos la carta al diccionario correspondiente\n        if palo in card_types:\n            if num not in card_types[palo]:\n                card_types[palo].append(num)\n            else:\n                print(\"Se encontraron cartas repetidas\")\n                invalid_cards += 1\n\n    envido = {}\n\n    # Comprobar si la cantidad de cartas es 3\n    if len(filtered_cards) != 3 or invalid_cards:\n        print('La cantidad de cartas no permite calcular el envido.\\n')\n\n    # 3 cartas en la detección\n    else:\n        # Inicializamos la variable palo\n\n        # calculamos el envido\n        # Iteramos en el diccionario - Key = palos / Value: números\n        for key, values in card_types.items():\n            envido[key]=0\n            # Dos cartas del mismo palo\n            if len(values) == 1:\n                envido[key] = int(num) if len(num)==1 else 0\n\n            elif len(values) == 2:\n                envido[key] = calcular_envido(values[0],values[1])\n\n            # Tres cartas del mismo palo\n            elif len(values) == 3:\n                print('Mano con Flor.\\n')\n                envido[key] = calcular_envido(values[0],values[1])\n                envido[key] = max(envido[key],calcular_envido(values[0],values[2]))\n                envido[key] = max(envido[key],calcular_envido(values[1],values[2]))\n\n\n    mejor_palo = 'N/A'\n    mejor_punto = 0\n\n    for palo, punto in envido.items():\n        if punto > mejor_punto:\n            mejor_punto=punto\n            mejor_palo = palo\n\n    # Almacenamos los resultados del envido en el diccionario para el JSON\n\n\n    card_js_file[img_filename] = {\n            'total_cards': len(filtered_cards),\n            'cards': card_types,\n            'points': mejor_punto,       # Puntos de envido\n            'figure': mejor_palo          # Palo\n        }\n\n\n# Marca de tiempo después de la inferencia\nend_time = time.time()\n\n# Calculamos el tiempo de ejecución\nexecution_time['rt'] = end_time - start_time\n\n# Mostramos que dispositivo se esta utilizando y el tiempo de ejecución\nif device.type == 'cuda': print('\\nEl modelo está utilizando la GPU.')\nelse: print('\\nEl modelo está utilizando la CPU.')\n# print(f'Tiempo de ejecución: {execution_time['cpu']:.2f} segundos')\nprint(f\"Tiempo de ejecución: {execution_time['rt']:.2f} segundos\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFFsNtYdwSOm","outputId":"fc6393ff-fc7d-415d-d7c6-899eefff6a3d","execution":{"iopub.status.busy":"2024-07-31T18:11:56.179159Z","iopub.execute_input":"2024-07-31T18:11:56.179521Z","iopub.status.idle":"2024-07-31T18:12:28.824208Z","shell.execute_reply.started":"2024-07-31T18:11:56.179485Z","shell.execute_reply":"2024-07-31T18:12:28.823299Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\nLoading /kaggle/working/model/weights/best.onnx for ONNX Runtime inference...\n\n","output_type":"stream"},{"name":"stderr","text":"\u001b[1;31m2024-07-31 18:11:56.299308943 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.11: cannot open shared object file: No such file or directory\n\u001b[m\n\u001b[0;93m2024-07-31 18:11:56.299339532 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n","output_type":"stream"},{"name":"stdout","text":"image 1/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173711753_HDR.jpg: 640x640 1 4O, 1 11C, 788.8ms\nimage 2/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173737533.jpg: 640x640 1 1C, 1 2B, 1 5C, 823.2ms\nimage 3/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173758691.jpg: 640x640 1 1C, 1 2B, 812.0ms\nimage 4/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173822184_HDR.jpg: 640x640 1 3B, 1 6B, 1 7E, 795.0ms\nimage 5/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173828513_HDR.jpg: 640x640 1 3B, 1 6B, 1 7E, 1 J, 785.2ms\nimage 6/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173836085.jpg: 640x640 1 6B, 1 7E, 783.6ms\nimage 7/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173859456.jpg: 640x640 1 3B, 790.9ms\nimage 8/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173918579.jpg: 640x640 1 8O, 1 9O, 780.4ms\nimage 9/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173929084.jpg: 640x640 1 3O, 1 8O, 1 9O, 786.3ms\nimage 10/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_173954348.jpg: 640x640 1 1O, 1 10C, 808.5ms\nimage 11/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174005265.jpg: 640x640 1 1O, 781.1ms\nimage 12/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174017809.jpg: 640x640 1 6E, 1 8B, 1 9B, 777.5ms\nimage 13/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174105714.jpg: 640x640 1 8B, 834.9ms\nimage 14/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174115784.jpg: 640x640 1 8C, 1 9C, 1 9E, 786.7ms\nimage 15/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174133804_HDR.jpg: 640x640 1 3E, 1 4E, 782.0ms\nimage 16/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174142929_HDR.jpg: 640x640 1 3E, 1190.3ms\nimage 17/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174155383_HDR.jpg: 640x640 1 11E, 1 12E, 1005.7ms\nimage 18/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174208092.jpg: 640x640 1 5E, 1 11O, 784.9ms\nimage 19/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174223161_HDR.jpg: 640x640 1 4O, 1 12B, 779.8ms\nimage 20/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174235296.jpg: 640x640 1 4O, 1 10E, 789.0ms\nimage 21/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174246178_HDR.jpg: 640x640 1 2C, 783.0ms\nimage 22/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174302382_HDR.jpg: 640x640 1 1E, 1 6C, 833.7ms\nimage 23/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174316833.jpg: 640x640 1 2O, 1 7C, 791.1ms\nimage 24/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174333447.jpg: 640x640 1 1B, 1 4B, 1 7B, 788.6ms\nimage 25/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174349395.jpg: 640x640 1 7O, 1 10B, 769.1ms\nimage 26/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174406256.jpg: 640x640 2 5Os, 1 10B, 768.8ms\nimage 27/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174418639.jpg: 640x640 2 5Os, 1 6O, 1 10B, 771.6ms\nimage 28/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174434126.jpg: 640x640 1 7C, 1 12O, 780.3ms\nimage 29/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174445046_HDR.jpg: 640x640 1 6O, 780.2ms\nimage 30/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174453153.jpg: 640x640 1 1B, 1 3C, 787.1ms\nimage 31/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174459285.jpg: 640x640 1 5B, 786.3ms\nimage 32/32 /kaggle/input/cv-tp-final/CV_TP_Final/data/eval/images/val/IMG_20240630_174514649_HDR.jpg: 640x640 1 10O, 1 11B, 1 12C, 770.0ms\nSpeed: 3.4ms preprocess, 808.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nMano con Flor.\n\nLa cantidad de cartas no permite calcular el envido.\n\nSe encontraron cartas repetidas\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\nLa cantidad de cartas no permite calcular el envido.\n\n\nEl modelo está utilizando la GPU.\nTiempo de ejecución: 32.63 segundos\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4.4. *Comparativa de Inferencias*","metadata":{"id":"HaokfCNTtD_P"}},{"cell_type":"code","source":"# Comparación de los tiempos de ejecución\nprint(\"Comparación de Tiempos de Ejecución:\")\nprint(f\"Tiempo de ejecución (CPU): {execution_time['cpu']:.2f} segundos\")\nprint(f\"Tiempo de ejecución (GPU): {execution_time['gpu']:.2f} segundos\")\nprint(f\"Tiempo de ejecución (TensorRT): {execution_time['rt']:.2f} segundos\")","metadata":{"id":"hLdMESYatKC3","outputId":"a009291b-e2ac-43dd-e9bd-aae841f25e1e","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-31T18:12:37.457449Z","iopub.execute_input":"2024-07-31T18:12:37.458052Z","iopub.status.idle":"2024-07-31T18:12:37.462867Z","shell.execute_reply.started":"2024-07-31T18:12:37.458022Z","shell.execute_reply":"2024-07-31T18:12:37.461953Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Comparación de Tiempos de Ejecución:\nTiempo de ejecución (CPU): 34.64 segundos\nTiempo de ejecución (GPU): 6.27 segundos\nTiempo de ejecución (TensorRT): 32.63 segundos\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5. **Guardamos las imagenes con las predicciones**","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"kTG_5YDThkYu"}},{"cell_type":"code","source":"# Cargamos el modelo\n# model = YOLO(model_path)              # Con CPU\nmodel = YOLO(model_path).to(device)     # Con GPU (sin TensorRT)\n# model = YOLO(model_path_tensor)       # Con GPU (con TensorRT)\n\n# Ejecutamos la inferencia\nresults = model(imgs_dir, stream=True)\n\n# Directorio donde van a ir las deteciones\ndetection_dir = f'{base_dir}/detections/'\n\n# Creamos el directorio si no existe\nos.makedirs(detection_dir, exist_ok=True)\n\n# Iteramos sobre los resultados\nfor result in results:\n    # Obtenemos el nombre del archivo de imagen\n    img_filename = os.path.basename(result.path)\n\n    # Guardamos la detección\n    result.save(filename=f'{detection_dir}/{img_filename}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjIxN-B3FBTK","outputId":"2801906a-b85e-4193-bbc5-be38ea01e27c","jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\nimage 1/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173711753_HDR.jpg: 640x512 1 4O, 1 11C, 1805.6ms\n\nimage 2/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173737533.jpg: 640x512 1 1C, 1 2B, 1 5C, 1304.6ms\n\nimage 3/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173758691.jpg: 640x512 1 1C, 1 2B, 1376.4ms\n\nimage 4/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173822184_HDR.jpg: 640x512 1 3B, 1 6B, 1 7E, 1357.8ms\n\nimage 5/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173828513_HDR.jpg: 640x512 1 3B, 1 6B, 1 7E, 1 J, 1363.4ms\n\nimage 6/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173836085.jpg: 640x512 1 6B, 1 7E, 1334.7ms\n\nimage 7/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173859456.jpg: 640x512 1 3B, 1530.4ms\n\nimage 8/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173918579.jpg: 640x512 1 8O, 1 9O, 1962.9ms\n\nimage 9/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173929084.jpg: 640x512 1 3O, 1 8O, 1 9O, 1728.0ms\n\nimage 10/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_173954348.jpg: 640x512 1 1O, 1 10C, 1383.8ms\n\nimage 11/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174005265.jpg: 640x512 1 1O, 1274.4ms\n\nimage 12/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174017809.jpg: 640x512 1 6E, 1 8B, 1 9B, 1267.8ms\n\nimage 13/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174105714.jpg: 640x512 1 8B, 1309.7ms\n\nimage 14/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174115784.jpg: 640x512 1 8C, 1 9C, 1 9E, 1302.1ms\n\nimage 15/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174133804_HDR.jpg: 640x512 1 3E, 1 4E, 1334.6ms\n\nimage 16/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174142929_HDR.jpg: 640x512 1 3E, 1898.2ms\n\nimage 17/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174155383_HDR.jpg: 640x512 1 11E, 1 12E, 1928.7ms\n\nimage 18/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174208092.jpg: 640x512 1 5E, 1 11O, 1320.6ms\n\nimage 19/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174223161_HDR.jpg: 640x512 1 4O, 1 12B, 1315.7ms\n\nimage 20/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174235296.jpg: 640x512 1 4O, 1 10E, 1325.5ms\n\nimage 21/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174246178_HDR.jpg: 640x512 1 2C, 1312.2ms\n\nimage 22/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174302382_HDR.jpg: 640x512 1 1E, 1 6C, 1302.8ms\n\nimage 23/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174316833.jpg: 640x512 1 2O, 1 7C, 1319.3ms\n\nimage 24/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174333447.jpg: 640x512 1 1B, 1 4B, 1 7B, 1888.1ms\n\nimage 25/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174349395.jpg: 640x512 1 7O, 1 10B, 1919.9ms\n\nimage 26/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174406256.jpg: 640x512 2 5Os, 1 10B, 1328.3ms\n\nimage 27/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174418639.jpg: 640x512 2 5Os, 1 6O, 1 10B, 1290.0ms\n\nimage 28/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174434126.jpg: 640x512 1 7C, 1 12O, 1305.3ms\n\nimage 29/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174445046_HDR.jpg: 640x512 1 6O, 1285.0ms\n\nimage 30/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174453153.jpg: 640x512 1 1B, 1 3C, 1272.7ms\n\nimage 31/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174459285.jpg: 512x640 1 5B, 1312.3ms\n\nimage 32/32 /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/eval/images/val/IMG_20240630_174514649_HDR.jpg: 640x512 1 10O, 1 11B, 1 12C, 1656.7ms\n\nSpeed: 4.2ms preprocess, 1456.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n"}]},{"cell_type":"markdown","source":"## 6. **Escritura del archivo envido.json**","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"vAkTjCfUhkYv"}},{"cell_type":"code","source":"with open(os.path.join(dets_dir, 'envido.json'), 'w') as jf:\n    json.dump(card_js_file, jf, indent=4)","metadata":{"id":"wc50R4WLhkYv","execution":{"iopub.status.busy":"2024-07-31T18:13:44.677957Z","iopub.execute_input":"2024-07-31T18:13:44.678736Z","iopub.status.idle":"2024-07-31T18:13:44.684755Z","shell.execute_reply.started":"2024-07-31T18:13:44.678703Z","shell.execute_reply":"2024-07-31T18:13:44.683809Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"## 7. **Exportar directorios**","metadata":{"id":"TInWEVkuhqDB"}},{"cell_type":"code","source":"# Especifica la carpeta a comprimir y el nombre del archivo zip\nfolder_path = '/content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/out/detections'\n\nzip_name = '/content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/out/detections.zip'\n\n# Crear el archivo zip\nshutil.make_archive(zip_name.replace('.zip', ''), 'zip', folder_path)\n\nprint(f\"Archivo zip creado: {zip_name}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T11:07:07.173550Z","iopub.execute_input":"2024-07-28T11:07:07.174541Z","iopub.status.idle":"2024-07-28T11:07:10.239276Z","shell.execute_reply.started":"2024-07-28T11:07:07.174505Z","shell.execute_reply":"2024-07-28T11:07:10.238323Z"},"id":"46C89uqlhqDD","outputId":"272d4909-8ca1-4e13-8727-3907e135c1e2","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":"Archivo zip creado: /content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/out/detections.zip\n"}]},{"cell_type":"code","source":"# Eliminar un archivo\nos.remove('/content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/out/detections.zip')\nprint(\"Eliminación completa!\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T11:12:05.215285Z","iopub.execute_input":"2024-07-28T11:12:05.216362Z","iopub.status.idle":"2024-07-28T11:12:05.300249Z","shell.execute_reply.started":"2024-07-28T11:12:05.216325Z","shell.execute_reply":"2024-07-28T11:12:05.299310Z"},"id":"DpfCoSY6hqDE","outputId":"7c7bede8-9f28-4681-edb9-24607a98d706","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":"Eliminación completa!\n"}]},{"cell_type":"code","source":"# Eliminar un directorio y todo su contenido\nshutil.rmtree('/content/drive/MyDrive/UNR/5 - Proc de Imág y Visión Comp. (IA52)/CV_TP_Final/data/out/detections')\nprint(\"Eliminación completa!\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T11:16:32.995292Z","iopub.execute_input":"2024-07-28T11:16:32.995705Z","iopub.status.idle":"2024-07-28T11:16:33.033287Z","shell.execute_reply.started":"2024-07-28T11:16:32.995674Z","shell.execute_reply":"2024-07-28T11:16:33.032382Z"},"id":"018JUVNUhqDF","outputId":"71a16335-c8fa-468a-de6e-a0b93b9cf2c8","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Eliminación completa!\n","output_type":"stream"}]}]}